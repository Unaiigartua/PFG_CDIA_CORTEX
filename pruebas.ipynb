{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de pruebas PFG CDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ICB-UMA/ClinLinker-KB-P\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0 if torch.cuda.is_available() else -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"El paciente presenta síntomas de diabetes mellitus tipo 2.\"\n",
    "\n",
    "# Obtener las entidades médicas detectadas\n",
    "entities = nlp_pipeline(text)\n",
    "\n",
    "# Mostrar resultados\n",
    "for entity in entities:\n",
    "    print(f\"Texto: {entity['word']}, Etiqueta: {entity['entity_group']}, Puntaje: {entity['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BSC-TeMU/roberta-base-biomedical-es\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"BSC-TeMU/roberta-base-biomedical-es\")\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model=\"BSC-TeMU/roberta-base-biomedical-es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker(\"El único antecedente personal a reseñar era la <mask> arterial.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker(\"El paciente presenta síntomas de <mask> mellitus tipo 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejores resultados\n",
    "https://huggingface.co/Helios9/BioMed_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_consecutive_entities(entities, text):\n",
    "    entities = sorted(entities, key=lambda x: x['start'])\n",
    "    merged_entities = []\n",
    "    current_entity = None\n",
    "\n",
    "    for entity in entities:\n",
    "        if current_entity is None:\n",
    "            current_entity = entity\n",
    "        elif (\n",
    "            entity['entity_group'] == current_entity['entity_group'] and\n",
    "            (entity['start'] <= current_entity['end'])\n",
    "        ):\n",
    "            # Merge based on start and end positions in the text\n",
    "            current_entity['end'] = max(current_entity['end'], entity['end'])\n",
    "            current_entity['word'] = text[current_entity['start']:current_entity['end']]\n",
    "            current_entity['score'] = (current_entity['score'] + entity['score']) / 2  \n",
    "        else:\n",
    "            merged_entities.append(current_entity)\n",
    "            current_entity = entity\n",
    "    if current_entity:\n",
    "        merged_entities.append(current_entity)\n",
    "\n",
    "    return merged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model\n",
    "model_path = \"Helios9/BIOMed_NER\"\n",
    "pipe = pipeline(\n",
    "    task=\"token-classification\",\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"patients diagnosed in the inner lower quadrant of breast that went under lumpeoctomy\")\n",
    "result = pipe(text)\n",
    "final_result=merge_consecutive_entities(result,text)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas en castellano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"lcampillos/roberta-es-clinical-trials-ner\", aggregation_strategy=\"simple\")\n",
    "texto = \"Pacientes femeninas diagnosticadas con carcinoma adenoescamoso de pulmón.\"\n",
    "entidades = ner_pipeline(texto)\n",
    "print(entidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=merge_consecutive_entities(entidades,texto)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda códigos para térmnos médicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera aproximación usando Snowstorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import quote\n",
    "import json\n",
    "\n",
    "baseUrl = 'https://snowstorm-training.snomedtools.org/snowstorm/snomed-ct'\n",
    "edition = 'MAIN'\n",
    "\n",
    "# IMPORTANT! You must update this user agent to avoid having your IP banned for 24 hours.\n",
    "# Replace with a contact email so that we can contact you if your script causes excessive load on the public server\n",
    "# For example: user_agent = 'example@example.com'\n",
    "user_agent = 'unaiigartua2@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlopen_with_header(url):\n",
    "    # adds User-Agent header otherwise urlopen on its own gets an IP blocked response\n",
    "    req = Request(url)\n",
    "    req.add_header('User-Agent', user_agent)\n",
    "    return urlopen(req)\n",
    "\n",
    "\n",
    "    \n",
    " #Prints snomed code for searched disease or symptom\n",
    "def getSnomedCodeSimilar(searchTerm):\n",
    "    #url = baseUrl + '/browser/' + edition + '/descriptions?term=' + quote(searchTerm) + '&conceptActive=true&groupByConcept=false&searchMode=STANDARD&offset=0&limit=50'\n",
    "    url = 'https://snowstorm-training.snomedtools.org/snowstorm/snomed-ct/browser/MAIN/descriptions?term=' + quote(searchTerm) + '&active=true&conceptActive=true&groupByConcept=false&searchMode=STANDARD&offset=0&limit=50'\n",
    "    print(url)\n",
    "    response = urlopen_with_header(url).read()\n",
    "    data = json.loads(response.decode('utf-8'))\n",
    "\n",
    "    # for term in data['items']:\n",
    "    #   print(term)  \n",
    "    #   if searchTerm in term['term']:\n",
    "    #     print(\"{} : {}\".format(term['term'], term['concept']['conceptId']))\n",
    "\n",
    "\n",
    "    diccionario = []\n",
    "    for term in data['items']:\n",
    "        print(term)\n",
    "        diccionario.append([term['term'], term['concept']['conceptId']])\n",
    "    \n",
    "    return diccionario\n",
    "\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = getSnomedCodeSimilar('lumpectomy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "model_name = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def get_mean_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask_expanded\n",
    "    summed = torch.sum(masked_embeddings, dim=1)\n",
    "    counts = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "    mean_pooled = summed / counts\n",
    "    return mean_pooled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de textos\n",
    "text1 = \"inner lower quadrant of breast\"\n",
    "text2 = \"Entire lower inner quadrant of breast\"\n",
    "\n",
    "emb1 = get_mean_embedding(text1)\n",
    "emb2 = get_mean_embedding(text2)\n",
    "\n",
    "similarity = cosine_similarity(emb1.numpy(), emb2.numpy())\n",
    "print(f\"Cosine Similarity: {similarity[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = get_mean_embedding(\"lumpectomy\")\n",
    "\n",
    "for term in terms:\n",
    "    emb2 = get_mean_embedding(term[0])\n",
    "    similarity = cosine_similarity(emb1.numpy(), emb2.numpy())\n",
    "    # añadir la similaridad al diccionario\n",
    "    term.append(similarity[0][0])\n",
    "\n",
    "print(terms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_terms = sorted(terms, key=lambda x: x[2], reverse=True)\n",
    "print(order_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = [item[0] for item in order_terms]\n",
    "valores = [item[2] for item in order_terms]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(valores, marker='o', linestyle='-')\n",
    "plt.xticks(range(len(nombres)), nombres, rotation=90)\n",
    "plt.xlabel(\"Términos\")\n",
    "plt.ylabel(\"Similaridad\")\n",
    "plt.title(\"Evolución de la Similaridad\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con lo anterior es que luego hay que pasar el código de SNOMED a OMOP. No hay ninguna api que lo permita de momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda aproximación\n",
    "Crear unos índices con todos los términos y sinónimos SNOMED y su equivalente a OMOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero hacer una base de datos de los códigos de Athena con la respectiva información de cada término"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar solo las columnas necesarias\n",
    "df = pd.read_csv(\"CONCEPT.csv\", sep='\\t', usecols=[\n",
    "    \"concept_id\", \"concept_name\", \"domain_id\", \"vocabulary_id\",\n",
    "    \"concept_class_id\", \"standard_concept\", \"concept_code\"\n",
    "])\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(\"omop_snomed.db\")\n",
    "df.to_sql(\"concepts\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "\n",
    "conn.execute(\"CREATE INDEX IF NOT EXISTS idx_code ON concepts(concept_code);\")\n",
    "conn.execute(\"CREATE INDEX IF NOT EXISTS idx_name ON concepts(concept_name);\")\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_concept_by_code(code: str, conn):\n",
    "    query = \"\"\"\n",
    "    SELECT concept_id, concept_name, domain_id, vocabulary_id, standard_concept\n",
    "    FROM concepts\n",
    "    WHERE concept_code = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(query, conn, params=(code,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a la base\n",
    "conn = sqlite3.connect(\"omop_snomed.db\")\n",
    "\n",
    "# Buscar por código SNOMED\n",
    "find_concept_by_code(\"392021009\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo, los índices para buscar por similaridad en los sinónimos de SNOMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "CSV_PATH = \"CONCEPT_SYNONYM.csv\"\n",
    "MODEL_NAME = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "\n",
    "\n",
    "FAISS_INDEX_PATH = \"faiss_snomed.index\"\n",
    "ID_MAPPING_PATH = \"concept_ids.pkl\"\n",
    "SYNONYMS_PATH = \"synonyms.parquet\"\n",
    "\n",
    "\n",
    "\n",
    "def build_vector_index():\n",
    "    df = pd.read_csv(CSV_PATH, sep=\"\\t\", usecols=[\"concept_id\", \"concept_synonym_name\"])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_parquet(SYNONYMS_PATH)\n",
    "\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    texts = df[\"concept_synonym_name\"].tolist()\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "    with open(ID_MAPPING_PATH, \"wb\") as f:\n",
    "        pickle.dump(df[\"concept_id\"].tolist(), f)\n",
    "\n",
    "\n",
    "def load_vector_index():\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "    with open(ID_MAPPING_PATH, \"rb\") as f:\n",
    "        concept_ids = pickle.load(f)\n",
    "    syn_df = pd.read_parquet(SYNONYMS_PATH)\n",
    "    return index, concept_ids, syn_df\n",
    "\n",
    "\n",
    "def search_synonym(text: str, k: int = 10) -> List[Tuple[int, str]]:\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    query_vec = model.encode([text]).astype(\"float32\")\n",
    "\n",
    "    index, concept_ids, syn_df = load_vector_index()\n",
    "    distances, indices = index.search(query_vec, k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        cid = concept_ids[idx]\n",
    "        synonym = syn_df.iloc[idx][\"concept_synonym_name\"]\n",
    "        results.append((cid, synonym))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NO EJECUTAR ESTO\n",
    "\n",
    "build_vector_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_synonym(\"inner lower quadrant of breast\", k=15)\n",
    "for cid, name in results:\n",
    "    print(f\" - {cid} → {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntar las dos partes anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_concept_by_code_OMOP(code: str, conn):\n",
    "    query = \"\"\"\n",
    "    SELECT concept_id, concept_name, domain_id, vocabulary_id, standard_concept\n",
    "    FROM concepts\n",
    "    WHERE concept_id = ?\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(query, conn, params=(code,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"omop_snomed.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_concept_by_code_OMOP(\"4078061\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid, name in results:\n",
    "    concept_info = find_concept_by_code_OMOP(cid, conn)\n",
    "    print(f\"Concept ID: {cid}, Concept Name: {name}, Preferred Name: {concept_info['concept_name'].values[0]}, Domain: {concept_info['domain_id'].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de un RAG para obtener preguntas similares y sus correspondientes SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script ragTest.py (pendiente pasarlo al notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de ejecución:\n",
    "\n",
    "python ragTest.py \"patients diagnosed in the inner lower quadrant of breast that went under lumpeoctomy\"\n",
    "\n",
    "[•] Query: patients diagnosed in the inner lower quadrant of breast that went under lumpeoctomy\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "#1 – score 0.357\n",
    "Q: How many women of reproductive age are taking estradiol? (by age group and year)\n",
    "\n",
    "SQL (truncated): WITH drug_women AS (\n",
    "    SELECT \n",
    "        p.person_id,\n",
    "        YEAR(de.drug_exposure_start_date) AS year,\n",
    "        YEAR(de.drug_exposure_start_date) - p.year_of_birth as age\n",
    "    FROM \n",
    "        PERSON p\n",
    " …\n",
    "\n",
    "―――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
    "\n",
    "\n",
    "Full SQL for best match:\n",
    "\n",
    "WITH drug_women AS (\n",
    "    SELECT \n",
    "        p.person_id,\n",
    "        YEAR(de.drug_exposure_start_date) AS year,\n",
    "        YEAR(de.drug_exposure_start_date) - p.year_of_birth as age\n",
    "    FROM \n",
    "        PERSON p\n",
    "    JOIN \n",
    "        DRUG_EXPOSURE de ON p.person_id = de.person_id\n",
    "    WHERE \n",
    "        p.gender_concept_id = 8532 AND\n",
    "        de.drug_concept_id IN (1548195,46287661,19093304,1548734,19109764,1548736,1548615,35603407,1548616,35603416,1548735,19084035,46287654,35603378,19082846,1548739,19121177,1548678,1548805,19081205,19086247,1356309,40169035,1548673,1559873,1548672,1548681,1356299,1548619,19121175,19109767,1548704,1548617,19110010,1548702,1548737,19100534,19103062,1548971,19117759,40181754,19087362,19043252,40181757,1548679)\n",
    "),\n",
    "age_groups AS (\n",
    "    SELECT \n",
    "        person_id,\n",
    "        year,\n",
    "        CASE\n",
    "            WHEN age BETWEEN 15 AND 20 THEN '15-20'\n",
    "            WHEN age BETWEEN 21 AND 24 THEN '21-24'\n",
    "            WHEN age BETWEEN 25 AND 34 THEN '25-34'\n",
    "            WHEN age BETWEEN 35 AND 44 THEN '35-44'\n",
    "            ELSE 'Other'\n",
    "        END AS age_group\n",
    "    FROM \n",
    "        drug_women\n",
    ")\n",
    "SELECT \n",
    "    year,\n",
    "    age_group,\n",
    "    COUNT(DISTINCT person_id) AS patient_count\n",
    "FROM \n",
    "    age_groups\n",
    "GROUP BY \n",
    "    year,\n",
    "    age_group\n",
    "ORDER BY \n",
    "    year,\n",
    "    age_group\n",
    "LIMIT 10000;\n",
    "\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "#2 – score 0.340\n",
    "Q: what are patients counts for sleeplessness while having menopause?\n",
    "\n",
    "SQL (truncated): WITH menopause_patients AS (\n",
    "    SELECT \n",
    "        p.person_id,\n",
    "        co.condition_start_date as menopause_start_date\n",
    "    FROM \n",
    "        PERSON p\n",
    "    JOIN \n",
    "        CONDITION_OCCURRENCE co ON p.person_i…\n",
    "\n",
    "―――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
    "\n",
    "#3 – score 0.314\n",
    "Q: What is the number of Afib patients getting the electric Cardioversion ? Break down by year (2000-2022)\n",
    "\n",
    "SQL (truncated): WITH afib_patients AS (\n",
    "    SELECT DISTINCT person_id\n",
    "    FROM condition_occurrence\n",
    "    WHERE condition_concept_id IN (313217,37395821,4119602,4119601,4064452,4139517,4154290,4108832,36713962,37394031…\n",
    "\n",
    "―――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
    "\n",
    "#4 – score 0.312\n",
    "Q: What is the 5-year bucket age distribution stratified by gender? Please calculate the age at time of first ICD10-CM H35.81 diagnosis\n",
    "\n",
    "SQL (truncated): WITH icd10cm_h3581 AS (\n",
    "    SELECT\n",
    "        DISTINCT person_id,\n",
    "        condition_start_date\n",
    "    FROM\n",
    "        condition_occurrence\n",
    "    JOIN\n",
    "        concept\n",
    "    ON\n",
    "        condition_occurrence.condition…\n",
    "\n",
    "―――――――――――――――――――――――――――――――――――――――――――――――――――――――\n",
    "\n",
    "#5 – score 0.306\n",
    "Q: For patients with ICD10-CM have H35.81 diagnosis, what are the fifty most common treatments/therapies (at any time point)?\n",
    "\n",
    "SQL (truncated): WITH icd10cm_h3581 AS (\n",
    "    SELECT\n",
    "        DISTINCT person_id\n",
    "    FROM\n",
    "        condition_occurrence\n",
    "    JOIN\n",
    "        concept\n",
    "    ON\n",
    "        condition_occurrence.condition_source_concept_id = concept.c…\n",
    "\n",
    "―――――――――――――――――――――――――――――――――――――――――――――――――――――――"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de las sentencias SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "# Define the model name and other parameters\n",
    "model_name = \"imsanjoykb/sqlCoder-Qwen2.5-8bit\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Enable faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt\n",
    "db_schema = open(\"omop_schema_stub.txt\", \"r\").read()\n",
    "\n",
    "OMOP_PROMPT = \"\"\"You are an expert epidemiology data analyst.\n",
    "Translate the following natural-language question into a **valid SQL query**\n",
    "for a database using the OMOP Common Data Model v5.3.\n",
    "\n",
    "### OMOP schema\n",
    "{db_schema}\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Similar questions\n",
    "Use the following examples to help you costruct the SQL query requested.\n",
    "{similar_q1}\n",
    "{similar_sql1}\n",
    "\n",
    "### SQL for {question}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_q1 = \"How many patients in age between 18 and 35 or in age between 40 and 75 have anemia?\"\n",
    "similar_sql1 = \"\"\"SELECT COUNT(DISTINCT p.person_id) AS num_patients\n",
    "FROM PERSON p\n",
    "JOIN CONDITION_ERA ce ON p.person_id = ce.person_id\n",
    "WHERE ((YEAR(ce.condition_era_start_date) - p.year_of_birth) BETWEEN 18 AND 35\n",
    "       OR (YEAR(ce.condition_era_end_date) - p.year_of_birth) BETWEEN 18 AND 35\n",
    "       OR (YEAR(ce.condition_era_start_date) - p.year_of_birth) BETWEEN 40 AND 75\n",
    "       OR (YEAR(ce.condition_era_end_date) - p.year_of_birth) BETWEEN 40 AND 75)\n",
    "  AND ce.condition_concept_id IN ([condition@anemia])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many patients younger than 20 or older than 40 suffered from hypertension?\"\n",
    "prompt = OMOP_PROMPT.format(db_schema=db_schema, question=question, similar_q1=similar_q1, similar_sql1=similar_sql1)\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "generated = model.generate(\n",
    "    **inputs,\n",
    "    streamer        = streamer,\n",
    "    max_new_tokens  = 500,\n",
    "    temperature     = 0.1,\n",
    "    top_p           = 0.95,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfg_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
